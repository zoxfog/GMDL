{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d93c8021-9fe1-4c72-a32d-8ecdf029ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5316a7d3-a9f3-4402-914f-8047c9636568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d9c20a-8d09-40f8-88ab-14da730abdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file_name):\n",
    "    mat = scipy.io.loadmat(file_name)\n",
    "    Xtrain = mat.get('Yt')\n",
    "    # for the bias\n",
    "    Xtrain = np.vstack([Xtrain, np.ones(Xtrain.shape[1])]) \n",
    "    Ytrain = mat.get('Ct')\n",
    "    Xtest = mat.get('Yv')\n",
    "    Xtest = np.vstack([Xtest, np.ones(Xtest.shape[1])])\n",
    "    Ytest = mat.get('Cv')\n",
    "    return Xtrain.T, Ytrain.T, Xtest.T, Ytest.T\n",
    "\n",
    "datasets = [\"GMMData\",\"PeaksData\",\"SwissRollData\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91d0813-cbe7-429c-9398-63176edbe794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(X, w, eta = True):\n",
    "    product_Xw = X.T @ w\n",
    "    if eta==True:\n",
    "        exp = np.exp(product_Xw - np.max(product_Xw ))\n",
    "    else:\n",
    "        exp = np.exp(product_Xw )\n",
    "    div = np.divide(exp, np.sum(exp, axis = 1).reshape(-1,1))\n",
    "    return div\n",
    "\n",
    "def softmax_loss(X, C, w, eta = True):\n",
    "    sm = softmax(X, w, eta = eta)\n",
    "    log  = np.log(sm)\n",
    "    m = len(X[0])\n",
    "    return -np.sum(C*log)/m  \n",
    "\n",
    "def softmax_loss_gradient_W(X, C, w, eta = True):\n",
    "    #cross entrophy loss wrt W\n",
    "    sm= softmax(X, w, eta = eta)\n",
    "    m = len(X[0])\n",
    "    gradient = (1/m)*X @ (sm - C)\n",
    "    return gradient\n",
    "\n",
    "def softmax_loss_gradient_X(X, C, w, eta = True):\n",
    "    #cross entrophy loss wrt X\n",
    "    sm= softmax(X, w, eta = eta)\n",
    "    m = len(X[0])\n",
    "    gradient = (1/m)*w @ (sm - C).T\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5046440-be8b-4e94-bc3e-2d4f17589b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    def forward(self,input):\n",
    "        pass\n",
    "    def backward(self,output_grad,lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529627b9-8df6-4f0e-ae1f-8330b79788b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.weights = 0.10 * np.random.randn(output_size,input_size)\n",
    "        self.biases = np.random.randn(output_size,1)\n",
    "        self.inputs = None\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        print('input dense'+str(self.inputs.shape))\n",
    "        self.output = np.dot(self.inputs,self.weights) + self.biases\n",
    "        print('dense'+str(self.output.shape))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,output_grad,lr):\n",
    "        #Gradients on parametrs\n",
    "        w_grad = np.dot(output_grad,self.inputs.T)\n",
    "        self.weights -= lr* w_grad\n",
    "        print('bias shape' + str(self.biases.shape))\n",
    "\n",
    "        print('output_grad shape' + str(output_grad.shape))\n",
    "\n",
    "        self.biases -= lr* output_grad\n",
    "        return np.dot(self.weights.T, output_grad)\n",
    "\n",
    "    \n",
    "class Softmax_Layer(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        self.weights = 0.10 * np.random.randn(output_size,input_size) \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        print('input - softmax'+str(self.inputs.shape))\n",
    "        self.output = softmax(inputs,self.weights.T)\n",
    "        \n",
    "        print('softmax'+str(self.output.shape))\n",
    "        return self.output\n",
    "    \n",
    "    def calc_loss(self, C):\n",
    "        #cross entrophy loss\n",
    "        return softmax_loss(self.output, C, self.weights.T)\n",
    "    \n",
    "    def backward(self,c,lr):\n",
    "        # the only backward that applies on inputs (because the softmax_loss_g.. making the softmax inside of it)\n",
    "        grad_W = softmax_loss_gradient_W(self.inputs, c, self.weights.T).T\n",
    "        grad_X = softmax_loss_gradient_X(self.inputs, c, self.weights.T)\n",
    "        \n",
    "        print('grad_W ' + str(grad_W.shape ))\n",
    "        print('self.weights' + str(self.weights.shape))\n",
    "\n",
    "        self.weights -= grad_W*lr\n",
    "        return grad_X\n",
    "        #the time reducing version from the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40b4f93-b166-4c98-ad70-683e2310b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self,activation,activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs        \n",
    "        print('tanh'+str(self.activation(self.inputs).shape))\n",
    "        return self.activation(self.inputs)\n",
    "    \n",
    "    def backward(self,output_grad,lr):\n",
    "        return np.multiply(output_grad,self.activation_prime(self.inputs))\n",
    "    \n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        tanh_prime = lambda x:1-np.tanh(x)**2\n",
    "        super().__init__(tanh,tanh_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4be817f9-b91e-463e-b3b5-318a85d9c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self,layers,lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        self.loss = []\n",
    "    \n",
    "    def forward(self,X,C):\n",
    "        '''\n",
    "        forward pass of the network\n",
    "        '''\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "            \n",
    "            \n",
    "        self.loss.append(self.layers[-1].calc_loss(C))\n",
    "        #the output without softmax(just the logits)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self,c):\n",
    "        '''\n",
    "        backward pass through the network + updating params\n",
    "        '''\n",
    "        g_x = self.layers[-1].backward(c,self.lr)\n",
    "        \n",
    "        for layer in np.flip(self.layers[:-1]):\n",
    "            print('g_x' + str(g_x.shape))\n",
    "            g_x = layer.backward(g_x, self.lr)\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c69c19-2cd7-4e64-b06d-f46824014d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Net(layers = [Dense(6,6),Tanh(),Softmax_Layer(6,5)], lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e231a5-c3da-40ac-9876-ad4cfd537540",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = data_loader(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c3be57a-05ef-4c90-a6ec-73e4f283dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = data_loader(datasets[0])\n",
    "examples_num, feature_num = Xtrain.shape\n",
    "labels_num = Ytrain.shape[1]\n",
    "\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589ad28b-d5e6-42ea-8a2b-bf112d711c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epocs = 100\n",
    "lr = 0.1\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8298d182-c7a0-428a-aa93-0e4f38dacb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(examples_num)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dd0760b-5715-4653-90b7-3e0da401a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 6)\n",
      "(25000, 5)\n",
      "(6250, 6)\n",
      "(6250, 5)\n"
     ]
    }
   ],
   "source": [
    "for i in [Xtrain, Ytrain, Xtest, Ytest]:\n",
    "    print(i.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d38b762-8b44-4175-9fed-bf65df73a86f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ef7776981b41839bd9880ea86209a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dense(1, 6)\n",
      "dense(6, 6)\n",
      "tanh(6, 6)\n",
      "input - softmax(6, 6)\n",
      "softmax(6, 5)\n",
      "grad_W (5, 6)\n",
      "self.weights(5, 6)\n",
      "g_x(6, 6)\n",
      "g_x(6, 6)\n",
      "bias shape(6, 1)\n",
      "output_grad shape(6, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (6,1) doesn't match the broadcast shape (6,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19220\\90530983.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#Backward+update nn params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminiy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#next batch...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19220\\2802893682.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, c)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'g_x'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mg_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19220\\237716221.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, output_grad, lr)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output_grad shape'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (6,1) doesn't match the broadcast shape (6,6)"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "for epoc in tqdm(range(epocs)):\n",
    "    # Shuffle train data\n",
    "    indices = np.arange(examples_num)\n",
    "    np.random.shuffle(indices)\n",
    "    Xtrain = Xtrain[indices]\n",
    "    Ytrain = Ytrain[indices]\n",
    "    i = 0\n",
    "    while i * batch_size < examples_num:\n",
    "        # Obtain minibatch\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min((i + 1) * batch_size, examples_num)\n",
    "        minix = Xtrain[batch_start:batch_end]\n",
    "        miniy = Ytrain[batch_start:batch_end]\n",
    "        \n",
    "        #forward pass\n",
    "        nn.forward(minix,miniy)\n",
    "\n",
    "        #Backward+update nn params\n",
    "        nn.backward(miniy)\n",
    "        \n",
    "        #next batch...\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
